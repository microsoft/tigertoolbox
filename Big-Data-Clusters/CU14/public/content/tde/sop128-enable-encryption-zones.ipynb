{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["SOP0128 - Enable HDFS Encryption zones in Big Data Clusters.\n","============================================================\n","\n","Description\n","-----------\n","\n","Use this notebook to patch configmaps after Big Data Cluster upgrade to\n","enable HDFS encryption zones for Encryption At Rest.\n","\n","Steps\n","-----\n","\n","### Instantiate Kubernetes client"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide_input"]},"outputs":[],"source":["# Instantiate the Python Kubernetes client into 'api' variable\n","\n","import os\n","from IPython.display import Markdown\n","\n","try:\n","    from kubernetes import client, config\n","    from kubernetes.stream import stream\n","except ImportError: \n","\n","    # Install the Kubernetes module\n","    import sys\n","    !{sys.executable} -m pip install kubernetes    \n","    \n","    try:\n","        from kubernetes import client, config\n","        from kubernetes.stream import stream\n","    except ImportError:\n","        display(Markdown(f'HINT: Use [SOP059 - Install Kubernetes Python module](../install/sop059-install-kubernetes-module.ipynb) to resolve this issue.'))\n","        raise\n","\n","if \"KUBERNETES_SERVICE_PORT\" in os.environ and \"KUBERNETES_SERVICE_HOST\" in os.environ:\n","    config.load_incluster_config()\n","else:\n","    try:\n","        config.load_kube_config()\n","    except:\n","        display(Markdown(f'HINT: Use [TSG118 - Configure Kubernetes config](../repair/tsg118-configure-kube-config.ipynb) to resolve this issue.'))\n","        raise\n","\n","api = client.CoreV1Api()\n","\n","print('Kubernetes client instantiated')"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Get the namespace for the big data cluster\n","\n","Get the namespace of the Big Data Cluster from the Kuberenetes API.\n","\n","**NOTE:**\n","\n","If there is more than one Big Data Cluster in the target Kubernetes\n","cluster, then either:\n","\n","-   set \\[0\\] to the correct value for the big data cluster.\n","-   set the environment variable AZDATA\\_NAMESPACE, before starting\n","    Azure Data Studio."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide_input"]},"outputs":[],"source":["# Place Kubernetes namespace name for BDC into 'namespace' variable\n","\n","if \"AZDATA_NAMESPACE\" in os.environ:\n","    namespace = os.environ[\"AZDATA_NAMESPACE\"]\n","else:\n","    try:\n","        namespace = api.list_namespace(label_selector='MSSQL_CLUSTER').items[0].metadata.name\n","    except IndexError:\n","        from IPython.display import Markdown\n","        display(Markdown(f'HINT: Use [TSG081 - Get namespaces (Kubernetes)](../monitor-k8s/tsg081-get-kubernetes-namespaces.ipynb) to resolve this issue.'))\n","        display(Markdown(f'HINT: Use [TSG010 - Get configuration contexts](../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb) to resolve this issue.'))\n","        display(Markdown(f'HINT: Use [SOP011 - Set kubernetes configuration context](../common/sop011-set-kubernetes-context.ipynb) to resolve this issue.'))\n","        raise\n","\n","print('The kubernetes namespace for your big data cluster is: ' + namespace)"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Common utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["import base64\n","import difflib\n","import json\n","import kubernetes.client\n","\n","master_pod_name = 'master-0'\n","hdfs_name_node_pod_name = 'nmnode-0-0'\n","storage_configmap_name = 'mssql-hadoop-storage-0-configmap'\n","\n","def run_command_on_pod(pod, container, command):\n","    return stream(api.connect_get_namespaced_pod_exec, pod, namespace, command=['/bin/sh', '-c', command], container=container, stderr=True, stdout=True)\n","\n","def get_security_json():\n","    # api.connect_get_namespaced_pod_exec returns malformed json with single quotes if output is a valid json\n","    #\n","    return json.loads(base64.b64decode(run_command_on_pod(\n","        pod=master_pod_name, container='mssql-server', command='cat /var/run/configmaps/cluster/security.json | base64')).decode('utf-8'))\n","\n","def is_ad_deployment():\n","    security_json = get_security_json()\n","\n","    if 'environmentVariables' in security_json:\n","        pod_vars = security_json['environmentVariables']\n","        if 'SECURITY_MODE' in pod_vars and pod_vars['SECURITY_MODE'] == 'ActiveDirectory':\n","            print ('The cluster is deployed with Active Directory!')\n","            return True\n","    return False\n","\n","def is_bdc_kms_provider_present():\n","    present_message = 'BDC key provider jar exists'\n","    absent_message = 'BDC key provider jar does not exist'\n","    jar_presence = run_command_on_pod(\n","        pod=hdfs_name_node_pod_name, container='hadoop', command=\n","            'ls -la /opt/hadoop/share/hadoop/common/keyprovider/bdc-hadoop-kms-provider-*.jar \u0026\u0026 ' \\\n","            f'echo \"{present_message}\" || echo \"{absent_message}\"')\n","    return present_message in jar_presence\n","\n","def get_host_fqdn(host):\n","    security_json = get_security_json()\n","\n","    if 'environmentVariables' in security_json:\n","        pod_vars = security_json['environmentVariables']\n","\n","        for domain_var in ['SUBDOMAIN', 'DOMAIN']:\n","            if domain_var in pod_vars and pod_vars[domain_var] != '':\n","                host = f'{host}.{pod_vars[domain_var]}'\n","    return host\n","\n","def configmap_exist(configmap_name):\n","    try:\n","        configmap = api.read_namespaced_config_map(name=configmap_name, namespace=namespace)\n","    except kubernetes.client.rest.ApiException:\n","        return False\n","    return True\n","\n","def read_configmap(configmap_name):\n","    return api.read_namespaced_config_map(name=configmap_name, namespace=namespace)\n","\n","def replace_configmap(configmap_name, patched_configmap):\n","    api.replace_namespaced_config_map(name=configmap_name, namespace=namespace, body=json.loads(patched_configmap)) # works!\n","\n","def can_configmap_patch_be_applied():\n","    if not is_ad_deployment():\n","        return False, 'Warning! Configmap patch can only enable HDFS encryption zone on BDC deployments with Active Directory! Current deployment is non-AD.'\n","    \n","    if not is_bdc_kms_provider_present():\n","        return False, 'Warning! Configmap patch can only enable HDFS encryption zone on BDC deployments with BDC key provider for KMS.'\n","\n","    if not configmap_exist(storage_configmap_name):\n","        return False, f'Warning! Configmap patch cannot be applied because BDC storage configmap {storage_configmap_name} does not exist.'\n","    \n","    return True, 'Ok'"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Patching configmaps `mssql-hadoop-storage-0-configmap`, `mssql-hadoop-sparkhead-configmap`, `mssql-hadoop-spark-0-configmap` after BDC upgrade from earlier versions to enable HDFS encryption zones support in Big Data Cluster."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["import re\n","import xml.etree.ElementTree as ET\n","\n","# Preserving existing comments in XML document\n","#\n","class CommentedTreeBuilder(ET.TreeBuilder):\n","    def comment(self, data):\n","        self.start(ET.Comment, {})\n","        self.data(data)\n","        self.end(ET.Comment)\n","\n","def patch_configmap_json(configmap_json):\n","    core_site = 'core-site.xml'\n","    cs = configmap_json['data'][core_site]\n","\n","    parser = ET.XMLParser(target=CommentedTreeBuilder())\n","    csxml = ET.fromstring(cs, parser=parser)\n","    props = [prop.find('value') for prop in csxml.findall(\n","        \"./property/name/..[name='hadoop.security.key.provider.path']\")]\n","\n","    if len(props) == 0:\n","        prop = ET.SubElement(csxml, 'property')\n","        name = ET.SubElement(prop, 'name')\n","        name.text = 'hadoop.security.key.provider.path'\n","        value = ET.SubElement(prop, 'value')\n","        key_provider_path_tag = value\n","    else:\n","        key_provider_path_tag = props[0]\n","\n","    # Get HDFS Name Node FQDN repeated 5 times so that HDFS clients will\n","    # retry when single name Name Node is unavailable.\n","    #\n","    repeat_fqdn_n_times = 5\n","    name_node_fqdn = get_host_fqdn(hdfs_name_node_pod_name)\n","    name_node_fqdn = ';'.join([name_node_fqdn] * repeat_fqdn_n_times)\n","\n","    key_provider_path_tag.text = f'kms://https@{name_node_fqdn}:9600/kms'\n","    cs = ET.tostring(csxml, encoding='utf-8').decode('utf-8')\n","    configmap_json['data'][core_site] = cs\n","\n","    kms_site = 'kms-site.xml.tmpl'\n","    ks = configmap_json['data'][kms_site]\n","\n","    kp_uri_regex = re.compile('(\u003cname\u003ehadoop.kms.key.provider.uri\u003c/name\u003e\\s*\u003cvalue\u003e\\s*)(.*)(\\s*\u003c/value\u003e)', re.MULTILINE)\n","\n","    hdfsvault_svc_fqdn = get_host_fqdn('hdfsvault-svc')\n","\n","    def replace_uri(match_obj):\n","        if match_obj.group(2) == 'jceks://file@/var/run/secrets/keystores/kms/kms.jceks':\n","            return match_obj.group(1) + f'bdc://https@{hdfsvault_svc_fqdn}' + match_obj.group(3)\n","        return match_obj.group(0)\n","\n","    ks = kp_uri_regex.sub(replace_uri, ks)\n","\n","    configmap_json['data'][kms_site] = ks\n","    return json.dumps(configmap_json, indent=4, sort_keys=True)\n","\n","def patch_configmap_if_needed(configmap_name):\n","    configmap = read_configmap(configmap_name)\n","\n","    # Fix for model objects not valid for serialization\n","    #\n","    configmap = kubernetes.client.ApiClient().sanitize_for_serialization(configmap)\n","    pretty_configmap = json.dumps(configmap, indent=4, sort_keys=True)\n","\n","    patched_configmap = patch_configmap_json(configmap)\n","\n","    diff_lines = [line for line in difflib.context_diff(pretty_configmap.split(), patched_configmap.split(), fromfile='before patch', tofile='after patch')]\n","    if len(diff_lines) \u003e 0:\n","        print(f'Diff between configmap {configmap_name} before and after patch:')\n","        for line in diff_lines:\n","            print(line)\n","\n","        replace_configmap(configmap_name, patched_configmap)\n","        print(f'Applying patch for configmap {configmap_name} to enable support of HDFS encryption zones with BDC key provider.')\n","    else:\n","        print(f'Configmap {configmap_name} is already patched to enable support of HDFS encryption zones with BDC key provider.')\n","\n","can_be_applied, reason = can_configmap_patch_be_applied()\n","if can_be_applied:\n","    for configmap_name in [storage_configmap_name, 'mssql-hadoop-sparkhead-configmap', 'mssql-hadoop-spark-0-configmap']:\n","        if configmap_exist(configmap_name):\n","            patch_configmap_if_needed(configmap_name)\n","        else:\n","            print(f'Skipping configmap {configmap_name} because it does not exist on this BDC deployment.')\n","else:\n","    print(reason)"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Restart pods to apply changes of configmaps\n","\n","### Common functions\n","\n","Define helper functions used in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide_input"]},"outputs":[],"source":["# Define `run` function for transient fault handling, suggestions on error, and scrolling updates on Windows\n","import sys\n","import os\n","import re\n","import platform\n","import shlex\n","import shutil\n","import datetime\n","\n","from subprocess import Popen, PIPE\n","from IPython.display import Markdown\n","\n","retry_hints = {} # Output in stderr known to be transient, therefore automatically retry\n","error_hints = {} # Output in stderr where a known SOP/TSG exists which will be HINTed for further help\n","install_hint = {} # The SOP to help install the executable if it cannot be found\n","\n","def run(cmd, return_output=False, no_output=False, retry_count=0, base64_decode=False, return_as_json=False, regex_mask=None):\n","    \"\"\"Run shell command, stream stdout, print stderr and optionally return output\n","\n","    NOTES:\n","\n","    1.  Commands that need this kind of ' quoting on Windows e.g.:\n","\n","            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='data-pool')].metadata.name}\n","\n","        Need to actually pass in as '\"':\n","\n","            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='\"'data-pool'\"')].metadata.name}\n","\n","        The ' quote approach, although correct when pasting into Windows cmd, will hang at the line:\n","        \n","            `iter(p.stdout.readline, b'')`\n","\n","        The shlex.split call does the right thing for each platform, just use the '\"' pattern for a '\n","    \"\"\"\n","    MAX_RETRIES = 5\n","    output = \"\"\n","    retry = False\n","\n","    # When running `azdata sql query` on Windows, replace any \\n in \"\"\" strings, with \" \", otherwise we see:\n","    #\n","    #    ('HY090', '[HY090] [Microsoft][ODBC Driver Manager] Invalid string or buffer length (0) (SQLExecDirectW)')\n","    #\n","    if platform.system() == \"Windows\" and cmd.startswith(\"azdata sql query\"):\n","        cmd = cmd.replace(\"\\n\", \" \")\n","\n","    # shlex.split is required on bash and for Windows paths with spaces\n","    #\n","    cmd_actual = shlex.split(cmd)\n","\n","    # Store this (i.e. kubectl, python etc.) to support binary context aware error_hints and retries\n","    #\n","    user_provided_exe_name = cmd_actual[0].lower()\n","\n","    # When running python, use the python in the ADS sandbox ({sys.executable})\n","    #\n","    if cmd.startswith(\"python \"):\n","        cmd_actual[0] = cmd_actual[0].replace(\"python\", sys.executable)\n","\n","        # On Mac, when ADS is not launched from terminal, LC_ALL may not be set, which causes pip installs to fail\n","        # with:\n","        #\n","        #    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4969: ordinal not in range(128)\n","        #\n","        # Setting it to a default value of \"en_US.UTF-8\" enables pip install to complete\n","        #\n","        if platform.system() == \"Darwin\" and \"LC_ALL\" not in os.environ:\n","            os.environ[\"LC_ALL\"] = \"en_US.UTF-8\"\n","\n","    # When running `kubectl`, if AZDATA_OPENSHIFT is set, use `oc`\n","    #\n","    if cmd.startswith(\"kubectl \") and \"AZDATA_OPENSHIFT\" in os.environ:\n","        cmd_actual[0] = cmd_actual[0].replace(\"kubectl\", \"oc\")\n","\n","    # To aid supportability, determine which binary file will actually be executed on the machine\n","    #\n","    which_binary = None\n","\n","    # Special case for CURL on Windows.  The version of CURL in Windows System32 does not work to\n","    # get JWT tokens, it returns \"(56) Failure when receiving data from the peer\".  If another instance\n","    # of CURL exists on the machine use that one.  (Unfortunately the curl.exe in System32 is almost\n","    # always the first curl.exe in the path, and it can't be uninstalled from System32, so here we\n","    # look for the 2nd installation of CURL in the path)\n","    if platform.system() == \"Windows\" and cmd.startswith(\"curl \"):\n","        path = os.getenv('PATH')\n","        for p in path.split(os.path.pathsep):\n","            p = os.path.join(p, \"curl.exe\")\n","            if os.path.exists(p) and os.access(p, os.X_OK):\n","                if p.lower().find(\"system32\") == -1:\n","                    cmd_actual[0] = p\n","                    which_binary = p\n","                    break\n","\n","    # Find the path based location (shutil.which) of the executable that will be run (and display it to aid supportability), this\n","    # seems to be required for .msi installs of azdata.cmd/az.cmd.  (otherwise Popen returns FileNotFound) \n","    #\n","    # NOTE: Bash needs cmd to be the list of the space separated values hence shlex.split.\n","    #\n","    if which_binary == None:\n","        which_binary = shutil.which(cmd_actual[0])\n","\n","    # Display an install HINT, so the user can click on a SOP to install the missing binary\n","    #\n","    if which_binary == None:\n","        print(f\"The path used to search for '{cmd_actual[0]}' was:\")\n","        print(sys.path)\n","\n","        if user_provided_exe_name in install_hint and install_hint[user_provided_exe_name] is not None:\n","            display(Markdown(f'HINT: Use [{install_hint[user_provided_exe_name][0]}]({install_hint[user_provided_exe_name][1]}) to resolve this issue.'))\n","\n","        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\")\n","    else:   \n","        cmd_actual[0] = which_binary\n","\n","    start_time = datetime.datetime.now().replace(microsecond=0)\n","\n","    cmd_display = cmd\n","    if regex_mask is not None:\n","        regex = re.compile(regex_mask)\n","        cmd_display = re.sub(regex, '******', cmd)\n","        \n","    print(f\"START: {cmd_display} @ {start_time} ({datetime.datetime.utcnow().replace(microsecond=0)} UTC)\")\n","    print(f\"       using: {which_binary} ({platform.system()} {platform.release()} on {platform.machine()})\")\n","    print(f\"       cwd: {os.getcwd()}\")\n","\n","    # Command-line tools such as CURL and AZDATA HDFS commands output\n","    # scrolling progress bars, which causes Jupyter to hang forever, to\n","    # workaround this, use no_output=True\n","    #\n","\n","    # Work around a infinite hang when a notebook generates a non-zero return code, break out, and do not wait\n","    #\n","    wait = True \n","\n","    try:\n","        if no_output:\n","            p = Popen(cmd_actual)\n","        else:\n","            p = Popen(cmd_actual, stdout=PIPE, stderr=PIPE, bufsize=1)\n","            with p.stdout:\n","                for line in iter(p.stdout.readline, b''):\n","                    line = line.decode()\n","                    if return_output:\n","                        output = output + line\n","                    else:\n","                        if cmd.startswith(\"azdata notebook run\"): # Hyperlink the .ipynb file\n","                            regex = re.compile('  \"(.*)\"\\: \"(.*)\"') \n","                            match = regex.match(line)\n","                            if match:\n","                                if match.group(1).find(\"HTML\") != -1:\n","                                    display(Markdown(f' - \"{match.group(1)}\": \"{match.group(2)}\"'))\n","                                else:\n","                                    display(Markdown(f' - \"{match.group(1)}\": \"[{match.group(2)}]({match.group(2)})\"'))\n","\n","                                    wait = False\n","                                    break # otherwise infinite hang, have not worked out why yet.\n","                        else:\n","                            print(line, end='')\n","\n","        if wait:\n","            p.wait()\n","    except FileNotFoundError as e:\n","        if install_hint is not None:\n","            display(Markdown(f'HINT: Use {install_hint} to resolve this issue.'))\n","\n","        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\") from e\n","\n","    exit_code_workaround = 0 # WORKAROUND: azdata hangs on exception from notebook on p.wait()\n","\n","    if not no_output:\n","        for line in iter(p.stderr.readline, b''):\n","            try:\n","                line_decoded = line.decode()\n","            except UnicodeDecodeError:\n","                # NOTE: Sometimes we get characters back that cannot be decoded(), e.g.\n","                #\n","                #   \\xa0\n","                #\n","                # For example see this in the response from `az group create`:\n","                #\n","                # ERROR: Get Token request returned http error: 400 and server \n","                # response: {\"error\":\"invalid_grant\",# \"error_description\":\"AADSTS700082: \n","                # The refresh token has expired due to inactivity.\\xa0The token was \n","                # issued on 2018-10-25T23:35:11.9832872Z\n","                #\n","                # which generates the exception:\n","                #\n","                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 179: invalid start byte\n","                #\n","                print(\"WARNING: Unable to decode stderr line, printing raw bytes:\")\n","                print(line)\n","                line_decoded = \"\"\n","                pass\n","            else:\n","\n","                # azdata emits a single empty line to stderr when doing an hdfs cp, don't\n","                # print this empty \"ERR:\" as it confuses.\n","                #\n","                if line_decoded == \"\":\n","                    continue\n","                \n","                print(f\"STDERR: {line_decoded}\", end='')\n","\n","                if line_decoded.startswith(\"An exception has occurred\") or line_decoded.startswith(\"ERROR: An error occurred while executing the following cell\"):\n","                    exit_code_workaround = 1\n","\n","                # inject HINTs to next TSG/SOP based on output in stderr\n","                #\n","                if user_provided_exe_name in error_hints:\n","                    for error_hint in error_hints[user_provided_exe_name]:\n","                        if line_decoded.find(error_hint[0]) != -1:\n","                            display(Markdown(f'HINT: Use [{error_hint[1]}]({error_hint[2]}) to resolve this issue.'))\n","\n","                # Verify if a transient error, if so automatically retry (recursive)\n","                #\n","                if user_provided_exe_name in retry_hints:\n","                    for retry_hint in retry_hints[user_provided_exe_name]:\n","                        if line_decoded.find(retry_hint) != -1:\n","                            if retry_count \u003c MAX_RETRIES:\n","                                print(f\"RETRY: {retry_count} (due to: {retry_hint})\")\n","                                retry_count = retry_count + 1\n","                                output = run(cmd, return_output=return_output, retry_count=retry_count)\n","\n","                                if return_output:\n","                                    if base64_decode:\n","                                        import base64\n","                                        return base64.b64decode(output).decode('utf-8')\n","                                    else:\n","                                        return output\n","\n","    elapsed = datetime.datetime.now().replace(microsecond=0) - start_time\n","\n","    # WORKAROUND: We avoid infinite hang above in the `azdata notebook run` failure case, by inferring success (from stdout output), so\n","    # don't wait here, if success known above\n","    #\n","    if wait: \n","        if p.returncode != 0:\n","            raise SystemExit(f'Shell command:\\n\\n\\t{cmd_display} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(p.returncode)}.\\n')\n","    else:\n","        if exit_code_workaround !=0 :\n","            raise SystemExit(f'Shell command:\\n\\n\\t{cmd_display} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(exit_code_workaround)}.\\n')\n","\n","    print(f'\\nSUCCESS: {elapsed}s elapsed.\\n')\n","\n","    if return_output:\n","        if base64_decode:\n","            import base64\n","            return base64.b64decode(output).decode('utf-8')\n","        else:\n","            return output\n","\n","\n","\n","# Hints for tool retry (on transient fault), known errors and install guide\n","#\n","retry_hints = {'azdata': ['Endpoint sql-server-master does not exist', 'Endpoint livy does not exist', 'Failed to get state for cluster', 'Endpoint webhdfs does not exist', 'Adaptive Server is unavailable or does not exist', 'Error: Address already in use', 'Login timeout expired (0) (SQLDriverConnect)', 'SSPI Provider: No Kerberos credentials available',  ], 'kubectl': ['A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',  ], 'python': [ ], }\n","error_hints = {'azdata': [['Please run \\'azdata login\\' to first authenticate', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['The token is expired', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Reason: Unauthorized', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Max retries exceeded with url: /api/v1/bdc/endpoints', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Look at the controller logs for more details', 'TSG027 - Observe cluster deployment', '../diagnose/tsg027-observe-bdc-create.ipynb'], ['provided port is already allocated', 'TSG062 - Get tail of all previous container logs for pods in BDC namespace', '../log-files/tsg062-tail-bdc-previous-container-logs.ipynb'], ['Create cluster failed since the existing namespace', 'SOP061 - Delete a big data cluster', '../install/sop061-delete-bdc.ipynb'], ['Failed to complete kube config setup', 'TSG067 - Failed to complete kube config setup', '../repair/tsg067-failed-to-complete-kube-config-setup.ipynb'], ['Data source name not found and no default driver specified', 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], ['Can\\'t open lib \\'ODBC Driver 17 for SQL Server', 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], ['Control plane upgrade failed. Failed to upgrade controller.', 'TSG108 - View the controller upgrade config map', '../diagnose/tsg108-controller-failed-to-upgrade.ipynb'], ['NameError: name \\'azdata_login_secret_name\\' is not defined', 'SOP013 - Create secret for azdata login (inside cluster)', '../common/sop013-create-secret-for-azdata-login.ipynb'], ['ERROR: No credentials were supplied, or the credentials were unavailable or inaccessible.', 'TSG124 - \\'No credentials were supplied\\' error from azdata login', '../repair/tsg124-no-credentials-were-supplied.ipynb'], ['Please accept the license terms to use this product through', 'TSG126 - azdata fails with \\'accept the license terms to use this product\\'', '../repair/tsg126-accept-license-terms.ipynb'],  ], 'kubectl': [['no such host', 'TSG010 - Get configuration contexts', '../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb'], ['No connection could be made because the target machine actively refused it', 'TSG056 - Kubectl fails with No connection could be made because the target machine actively refused it', '../repair/tsg056-kubectl-no-connection-could-be-made.ipynb'],  ], 'python': [['Library not loaded: /usr/local/opt/unixodbc', 'SOP012 - Install unixodbc for Mac', '../install/sop012-brew-install-odbc-for-sql-server.ipynb'], ['WARNING: You are using pip version', 'SOP040 - Upgrade pip in ADS Python sandbox', '../install/sop040-upgrade-pip.ipynb'],  ], }\n","install_hint = {'azdata': [ 'SOP063 - Install azdata CLI (using package manager)', '../install/sop063-packman-install-azdata.ipynb' ],  'kubectl': [ 'SOP036 - Install kubectl command line interface', '../install/sop036-install-kubectl.ipynb' ],  }\n","\n","\n","print('Common functions defined successfully.')"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Parameters for waiting for healthy cluster state"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["parameters"]},"outputs":[],"source":["timeout = 600  # amount of time in seconds to wait before cluster is healthy:  default to 10 minutes\n","check_interval = 30  # amount of time in seconds between health checks - default 30 seconds\n","min_pod_count = 10  # minimum number of healthy pods required to assert health"]},{"cell_type":"markdown","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["### Helper functions for waiting for the cluster to become healthy"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide_input"]},"outputs":[],"source":["import threading\n","import time\n","import sys\n","import os\n","from IPython.display import Markdown\n","\n","isRunning = True\n","\n","def all_containers_ready(pod):\n","    \"\"\"helper method returns true if all the containers within the given pod are ready\n","\n","    Arguments:\n","        pod {v1Pod} -- Metadata retrieved from the api call to.\n","    \"\"\"\n","         \n","    return all(map(lambda c: c.ready is True, pod.status.container_statuses))\n","\n","\n","def pod_is_ready(pod):\n","    \"\"\"tests that the pod, and all containers are ready\n","\n","    Arguments:\n","        pod {v1Pod} -- Metadata retrieved from api call.\n","    \"\"\"\n","\n","    return \"job-name\" in pod.metadata.labels or (pod.status.phase == \"Running\" and all_containers_ready(pod))\n","\n","\n","def waitReady():\n","    \"\"\"Waits for all pods, and containers to become ready.\n","    \"\"\"\n","    while isRunning:\n","        try:\n","            time.sleep(check_interval)\n","            pods = get_pods()\n","            allReady = len(pods.items) \u003e= min_pod_count and all(map(pod_is_ready, pods.items))\n","\n","            if allReady:\n","                return True\n","            else:\n","                display(Markdown(get_pod_failures(pods)))\n","                display(Markdown(f\"cluster not healthy, rechecking in {check_interval} seconds.\"))\n","        except Exception as ex:\n","            last_error_message = str(ex)\n","            display(Markdown(last_error_message))\n","            time.sleep(check_interval)\n","\n","def get_pod_failures(pods=None):\n","    \"\"\"Returns a status message for any pods that are not ready.\n","    \"\"\"\n","    results = \"\"\n","    if not pods:\n","        pods = get_pods()\n","\n","    for pod in pods.items:\n","        if \"job-name\" not in pod.metadata.labels:\n","            if pod.status and pod.status.container_statuses:\n","                for container in filter(lambda c: c.ready is False, pod.status.container_statuses):\n","                    results = results + \"Container {0} in Pod {1} is not ready. Reported status: {2} \u003cbr/\u003e\".format(container.name, pod.metadata.name, container.state)       \n","            else:\n","                results = results + \"Pod {0} is not ready.  \u003cbr/\u003e\".format(pod.metadata.name)\n","    return results\n","\n","\n","def get_pods():\n","    \"\"\"Returns a list of pods by namespace, or all namespaces if no namespace is specified\n","    \"\"\"\n","    pods = None\n","    if namespace is not None:\n","        display(Markdown(f'Checking namespace {namespace}'))\n","        pods = api.list_namespaced_pod(namespace, _request_timeout=30) \n","    else:\n","        display(Markdown('Checking all namespaces'))\n","        pods = api.list_pod_for_all_namespaces(_request_timeout=30)\n","    return pods\n","\n","def wait_for_cluster_healthy():\n","    isRunning = True\n","    mt = threading.Thread(target=waitReady)\n","    mt.start()\n","    mt.join(timeout=timeout)\n","\n","    if mt.is_alive():\n","      raise SystemExit(\"Timeout waiting for all cluster to be healthy.\")\n","      \n","    isRunning = False"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["import re\n","\n","def restart_pods_if_needed():\n","    # Prevent unnecessary pod restart on deployments where HDFS encryption zones cannot be enabled.\n","    #\n","    can_be_applied, reason = can_configmap_patch_be_applied()\n","    if not can_be_applied:\n","        print(reason)\n","        return\n","\n","    pods_list_string = run(f'kubectl get pod -n {namespace} -o=name', return_output=True)\n","\n","    # Remember count of healthy pods\n","    #\n","    min_pod_count = len(get_pods().items)\n","\n","    pod_regex = re.compile('pod/(?P\u003cpod\u003e(nmnode-0|storage-0|sparkhead|spark-0)-[0-9]+)\\s*')\n","    pods = (pod_match.group('pod') for pod_match in pod_regex.finditer(pods_list_string))\n","    for pod in pods:\n","        print(f'Restarting pod {pod}')\n","        run(f'kubectl delete pod {pod} -n {namespace}')\n","\n","        # Do not restart more than one pod at once\n","        #\n","        wait_for_cluster_healthy()\n","\n","    print(\"Restarted pods. HDFS Encryption zones can now be used in the cluster.\")\n","    \n","restart_pods_if_needed()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["print(\"Notebook execution is complete.\")"]}],"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"pansop":{"related":"","test":{"strategy":"","types":null,"disable":{"reason":"","workitems":null,"types":null}},"target":{"current":"","final":""},"internal":{"parameters":null,"symlink":false},"timeout":"0"},"language_info":{"codemirror_mode":"{ Name: \"\", Version: \"\"}","file_extension":"","mimetype":"","name":"","nbconvert_exporter":"","pygments_lexer":"","version":""},"widgets":[]}}